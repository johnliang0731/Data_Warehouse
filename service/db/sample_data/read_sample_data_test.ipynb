{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/Users/mengruf/Documents/Github/OMSCS/CS6400_FALL20/project/cs6400-2020-03-Team040/Phase_3/service/db\n",
      "/Users/mengruf/Documents/Github/OMSCS/CS6400_FALL20/project/cs6400-2020-03-Team040/Phase_3/service/db/Sample Data\n",
      "['store.tsv', 'product.tsv', 'product_sale_prices.tsv', 'category.tsv', 'city.tsv', 'manufacturer.tsv', 'membership.tsv', 'holiday.tsv', 'product_sales.tsv', 'date.tsv']\n",
      "   storeid           phone                        address     cityname state\n",
      "0        1  (239) 167-4627              78 Fabien Freeway      Oakland    NH\n",
      "1        2  (744) 032-1717             29 West Milton St.  New Orleans    IN\n",
      "2        3    544-510-8934             271 Fabien Parkway      Lubbock    IL\n",
      "3        4    340-264-5348  444 North Rocky Milton Avenue    Cleveland    CT\n",
      "4        5  (111) 026-2512                 97 Hague Blvd.        Tampa    NE\n",
      "   productid           name                 Manufacturer  retailprice  \\\n",
      "0          1       Zeejubin    Bartanefin International        258.36   \n",
      "1          2  Varpickilower            Frojubupax  Corp.       314.78   \n",
      "2          3      Unwerplar  Emjubex International Corp.       256.45   \n",
      "3          4   Parfropollin   Surjubackar International        472.04   \n",
      "4          5     Inkilentor     Monhupefantor WorldWide        180.03   \n",
      "\n",
      "                                    categories  \n",
      "0                           Scanner,Video Game  \n",
      "1                        Air Conditioner,Dryer  \n",
      "2  Recording Equipment,Satellite Radio,Speaker  \n",
      "3                                       Camera  \n",
      "4                     GPS,Microwave,Video Game  \n",
      "   productid        date  saleprice\n",
      "0          1  2002-09-23     125.43\n",
      "1          1  2005-01-02     164.55\n",
      "2          2  2000-04-09     239.68\n",
      "3          2  2007-03-23     208.72\n",
      "4          3  2009-10-05     183.21\n",
      "              Name\n",
      "0  Air Conditioner\n",
      "1     Air Purifier\n",
      "2          Battery\n",
      "3           Bluray\n",
      "4           Camera\n",
      "          Name State  Population\n",
      "0        Akron    MO     7684915\n",
      "1        Akron    OH     5947796\n",
      "2        Akron    RI     3878285\n",
      "3  Albuquerque    NV     3608365\n",
      "4  Albuquerque    OR     6610173\n",
      "                                   name  maxdiscount\n",
      "0                  Admunaquax Holdings          0.11\n",
      "1  Klisapeficator International Company         0.00\n",
      "2                     Rehupadar Direct          0.68\n",
      "3                Ciptinicator Holdings          0.02\n",
      "4                Emtinicator WorldWide          0.16\n",
      "   membershipid membershiptype  storeid        date\n",
      "0        100000  Yellow Jacket      437  2002-09-03\n",
      "1        100001  Yellow Jacket      150  2011-01-18\n",
      "2        100002   Giant Hornet      114  2000-12-25\n",
      "3        100003  Yellow Jacket      265  2012-03-30\n",
      "4        100004  Yellow Jacket      506  2008-09-14\n",
      "         date        holidayname\n",
      "0  2011-01-01     New Year's Day\n",
      "1  2011-01-17  Martin Luther Day\n",
      "2  2011-02-21    President's Day\n",
      "3  2011-05-30       Memorial Day\n",
      "4  2011-07-04   Independence Day\n",
      "   productid  storeid        date  quantity\n",
      "0          1      108  2007-01-21         1\n",
      "1          1      214  2002-09-02         7\n",
      "2          1      234  2002-05-11         2\n",
      "3          1      242  2000-09-14         1\n",
      "4          1      315  2005-02-27         3\n",
      "         date\n",
      "0  2000-01-01\n",
      "1  2000-01-02\n",
      "2  2000-01-03\n",
      "3  2000-01-04\n",
      "4  2000-01-05\n",
      "['store.tsv', 'product.tsv', 'product_sale_prices.tsv', 'category.tsv', 'city.tsv', 'manufacturer.tsv', 'membership.tsv', 'holiday.tsv', 'product_sales.tsv', 'date.tsv']\n",
      "[1000, 25300, 57505, 29, 250, 1000, 400000, 15, 500000, 4566]\n"
     ]
    }
   ],
   "source": [
    "root_dir = os.path.dirname(os.getcwd())\n",
    "print(root_dir)\n",
    "orig_sample_data_dir = os.path.join(root_dir, 'Sample Data')\n",
    "print(orig_sample_data_dir)\n",
    "tsv_files = [f for f in os.listdir(orig_sample_data_dir) if f.split('.', 1)[1] == 'tsv']\n",
    "print(tsv_files)\n",
    "\n",
    "tsv_file_length = []\n",
    "for f in tsv_files:\n",
    "    df = f.split('.', 1)[0]\n",
    "    df = pd.read_csv(os.path.join(orig_sample_data_dir, f), sep='\\t')\n",
    "    tsv_file_length.append(df.shape[0])\n",
    "    print(df.head())\n",
    "\n",
    "print(tsv_files)\n",
    "print(tsv_file_length)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "   dateID   date_time\n0       0  2000-01-01\n1       1  2000-01-02\n2       2  2000-01-03\n3       3  2000-01-04\n4       4  2000-01-05\n4566\n"
     ]
    }
   ],
   "source": [
    "# Date.csv \n",
    "def transform_date_data(input_file): \n",
    "    curr_dir = os.getcwd()\n",
    "    root_dir = os.path.dirname(os.getcwd())\n",
    "    orig_sample_data_dir = os.path.join(root_dir, 'Sample Data')\n",
    "    # read tsv file\n",
    "    df = pd.read_csv(os.path.join(orig_sample_data_dir, input_file), sep='\\t')\n",
    "    # use index as dateID\n",
    "    df['dateID'] = df.index\n",
    "    # rename column name\n",
    "    df = df.rename({'date': 'date_time', 'dateID': 'dateID'}, axis=1)\n",
    "    # sort columns\n",
    "    df = df[['dateID', 'date_time']]\n",
    "\n",
    "    print(df.head())\n",
    "    print(df.shape[0])\n",
    "\n",
    "    # write into sample_data folder\n",
    "    df.to_csv(os.path.join(curr_dir, 'Date.csv'), index=False)\n",
    "    \n",
    "transform_date_data('date.tsv')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "      dateID   date_time\n",
       "0          0  2000-01-01\n",
       "1          1  2000-01-02\n",
       "2          2  2000-01-03\n",
       "3          3  2000-01-04\n",
       "4          4  2000-01-05\n",
       "...      ...         ...\n",
       "4561    4561  2012-06-27\n",
       "4562    4562  2012-06-28\n",
       "4563    4563  2012-06-29\n",
       "4564    4564  2012-06-30\n",
       "4565    4565  2012-07-01\n",
       "\n",
       "[4566 rows x 2 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>dateID</th>\n      <th>date_time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>0</td>\n      <td>2000-01-01</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>1</td>\n      <td>2000-01-02</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>2</td>\n      <td>2000-01-03</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>3</td>\n      <td>2000-01-04</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>4</td>\n      <td>2000-01-05</td>\n    </tr>\n    <tr>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <td>4561</td>\n      <td>4561</td>\n      <td>2012-06-27</td>\n    </tr>\n    <tr>\n      <td>4562</td>\n      <td>4562</td>\n      <td>2012-06-28</td>\n    </tr>\n    <tr>\n      <td>4563</td>\n      <td>4563</td>\n      <td>2012-06-29</td>\n    </tr>\n    <tr>\n      <td>4564</td>\n      <td>4564</td>\n      <td>2012-06-30</td>\n    </tr>\n    <tr>\n      <td>4565</td>\n      <td>4565</td>\n      <td>2012-07-01</td>\n    </tr>\n  </tbody>\n</table>\n<p>4566 rows Ã— 2 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "# Create Date Map\n",
    "def output_date_data(input_file): \n",
    "    curr_dir = os.getcwd()\n",
    "    root_dir = os.path.dirname(os.getcwd())\n",
    "    orig_sample_data_dir = os.path.join(root_dir, 'Sample Data')\n",
    "    # read tsv file\n",
    "    df = pd.read_csv(os.path.join(orig_sample_data_dir, input_file), sep='\\t')\n",
    "    # use index as dateID\n",
    "    df['dateID'] = df.index\n",
    "    # rename columns\n",
    "    df = df.rename({'date': 'date_time', 'dateID': 'dateID'}, axis=1)\n",
    "    # sort columns\n",
    "    df = df[['dateID', 'date_time']]\n",
    "    \n",
    "    return df \n",
    "\n",
    "output_date_data('date.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "   dateID       holiday_name\n0    4018     New Year's Day\n1    4034  Martin Luther Day\n2    4069    President's Day\n3    4167       Memorial Day\n4    4202   Independence Day\n15\n   dateID\n0       0\n1       1\n2       2\n3       3\n4       4\n4551\n"
     ]
    }
   ],
   "source": [
    "# Holiday.csv, NonHoliday.csv\n",
    "def transform_holiday_data(input_file, date_map): \n",
    "     curr_dir = os.getcwd()\n",
    "     root_dir = os.path.dirname(os.getcwd())\n",
    "     orig_sample_data_dir = os.path.join(root_dir, 'Sample Data')\n",
    "     # read tsv file\n",
    "     df = pd.read_csv(os.path.join(orig_sample_data_dir, input_file), sep='\\t')\n",
    "     # rename columns\n",
    "     df = df.rename({'date': 'date_time', 'holidayname': 'holiday_name'}, axis=1)\n",
    "     # map dateID column from date data\n",
    "     df_merged = pd.merge(df, date_map, on = ['date_time'], how = 'left')\n",
    "     # remove date_time column\n",
    "     df_holiday = df_merged[['dateID', 'holiday_name']]\n",
    "\n",
    "     print(df_holiday.head())\n",
    "     print(df_holiday.shape[0])\n",
    "\n",
    "     # write into sample_data folder\n",
    "     df_holiday.to_csv(os.path.join(curr_dir, 'Holiday.csv'), index=False)\n",
    "\n",
    "     # nonholiday \n",
    "     df_nonholiday = pd.merge(date_map, df, on = ['date_time'], how = 'left')\n",
    "     df_nonholiday = df_nonholiday.loc[df_nonholiday['holiday_name'].isnull(), :]\n",
    "     df_nonholiday = df_nonholiday[['dateID']]\n",
    "\n",
    "     print(df_nonholiday.head())\n",
    "     print(df_nonholiday.shape[0])\n",
    "     \n",
    "     # write into sample_data folder\n",
    "     df_nonholiday.to_csv(os.path.join(curr_dir, 'NonHoliday.csv'), index=False) \n",
    "     \n",
    "\n",
    "     \n",
    "\n",
    "transform_holiday_data('holiday.tsv', output_date_data('date.tsv'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "   store_number  dateID  PID  quantity\n0           108    2577    1         1\n1           214     975    1         7\n2           234     861    1         2\n3           242     257    1         1\n4           315    1884    1         3\n500000\n"
     ]
    }
   ],
   "source": [
    "# Sold.csv \n",
    "def transform_sold_data(input_file, date_map): \n",
    "    curr_dir = os.getcwd()\n",
    "    root_dir = os.path.dirname(os.getcwd())\n",
    "    orig_sample_data_dir = os.path.join(root_dir, 'Sample Data')\n",
    "    # read tsv file\n",
    "    df = pd.read_csv(os.path.join(orig_sample_data_dir, input_file), sep='\\t')\n",
    "    # rename columns \n",
    "    df = df.rename({'productid': 'PID', 'storeid': 'store_number', 'date': 'date_time', 'quantity': 'quantity'}, axis=1)\n",
    "    # map dateID column from date data\n",
    "    df_merged = pd.merge(df, date_map, on = ['date_time'], how = 'left')\n",
    "    # remove date_time column\n",
    "    df_truncated = df_merged[['store_number', 'dateID', 'PID', 'quantity']]\n",
    "    \n",
    "    print(df_truncated.head())\n",
    "    print(df_truncated.shape[0])\n",
    "\n",
    "    # write into sample_data folder\n",
    "    df_truncated.to_csv(os.path.join(curr_dir, 'Sold.csv'), index=False)\n",
    "\n",
    "transform_sold_data('product_sales.tsv', output_date_data('date.tsv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "   memberID  signup_date  store_number\n0    100000          976           437\n1    100001         4035           150\n2    100002          359           114\n3    100003         4472           265\n4    100004         3179           506\n400000\n   memberID\n0    100000\n1    100001\n3    100003\n4    100004\n5    100005\n300041\n    memberID\n2     100002\n6     100006\n7     100007\n11    100011\n15    100015\n99959\n"
     ]
    }
   ],
   "source": [
    "# Membership.csv, YellowJacket.csv, GiantHornet.csv\n",
    "def transform_membership_data(input_file, date_map): \n",
    "    curr_dir = os.getcwd()\n",
    "    root_dir = os.path.dirname(os.getcwd())\n",
    "    orig_sample_data_dir = os.path.join(root_dir, 'Sample Data')\n",
    "    # read tsv file\n",
    "    df = pd.read_csv(os.path.join(orig_sample_data_dir, input_file), sep='\\t')\n",
    "    # rename columns \n",
    "    df = df.rename({'membershipid': 'memberID', 'membershiptype': 'membershiptype', 'storeid': 'store_number', 'date': 'date_time'}, axis=1)\n",
    "    # map dateID column from date data\n",
    "    df_merged = pd.merge(df, date_map, on = ['date_time'], how = 'left')\n",
    "    # remove date_time column\n",
    "    df_membership = df_merged[['memberID', 'dateID', 'store_number']]\n",
    "    # rename dateID to signup_date \n",
    "    df_membership = df_membership.rename({'dateID': 'signup_date'}, axis=1)\n",
    "\n",
    "    # subclass: YellowJacket, GiantHornet\n",
    "    df_yellowjacket = df_merged.loc[df_merged['membershiptype'] == 'Yellow Jacket', ['memberID']]\n",
    "    df_gianthornet = df_merged.loc[df_merged['membershiptype'] == 'Giant Hornet', ['memberID']]\n",
    "\n",
    "    print(df_membership.head())\n",
    "    print(df_membership.shape[0])\n",
    "    print(df_yellowjacket.head())\n",
    "    print(df_yellowjacket.shape[0])\n",
    "    print(df_gianthornet.head())\n",
    "    print(df_gianthornet.shape[0])\n",
    "\n",
    "    # write into sample_data folder\n",
    "    df_membership.to_csv(os.path.join(curr_dir, 'Membership.csv'), index=False)\n",
    "    df_yellowjacket.to_csv(os.path.join(curr_dir, 'YellowJacket.csv'), index=False)\n",
    "    df_gianthornet.to_csv(os.path.join(curr_dir, 'GiantHornet.csv'), index=False)\n",
    "\n",
    "transform_membership_data('membership.tsv', output_date_data('date.tsv'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "   membershipid membershiptype  storeid        date\n0        100000  Yellow Jacket      437  2002-09-03\n1        100001  Yellow Jacket      150  2011-01-18\n2        100002   Giant Hornet      114  2000-12-25\n3        100003  Yellow Jacket      265  2012-03-30\n4        100004  Yellow Jacket      506  2008-09-14\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "membershiptype\n",
       "Giant Hornet      99959\n",
       "Yellow Jacket    300041\n",
       "Name: membershipid, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "# validate how many YellowJacket and GiantHornet in original tsv file \n",
    "membership_df = pd.read_csv(os.path.join(orig_sample_data_dir, 'membership.tsv'), sep='\\t')\n",
    "print(membership_df.head())\n",
    "\n",
    "membership_df.groupby('membershiptype')['membershipid'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(499023, 3)\n",
      "   productid        date  retailprice\n",
      "0          1  2007-01-21       258.36\n",
      "1          1  2002-09-02       258.36\n",
      "2          1  2002-05-11       258.36\n",
      "3          1  2000-09-14       258.36\n",
      "4          1  2005-02-27       258.36\n",
      "   PID  dateID  discount_percentage\n",
      "0    1    2577                  0.0\n",
      "1    1     975                  0.0\n",
      "2    1     861                  0.0\n",
      "3    1     257                  0.0\n",
      "4    1    1884                  0.0\n",
      "499023\n",
      "469515\n",
      "29508\n"
     ]
    }
   ],
   "source": [
    "# OnSale.csv\n",
    "def transform_onsale_data(input_file1, input_file2, input_file3, date_map): \n",
    "    curr_dir = os.getcwd()\n",
    "    root_dir = os.path.dirname(os.getcwd())\n",
    "    orig_sample_data_dir = os.path.join(root_dir, 'Sample Data')\n",
    "    # read tsv file\n",
    "    df1 = pd.read_csv(os.path.join(orig_sample_data_dir, input_file1), sep='\\t')\n",
    "    df2 = pd.read_csv(os.path.join(orig_sample_data_dir, input_file2), sep='\\t')\n",
    "    df3 = pd.read_csv(os.path.join(orig_sample_data_dir, input_file3), sep='\\t')\n",
    "\n",
    "    # only keep sold product\n",
    "    df_sold = pd.merge(df1, df3, on = ['productid'], how = 'inner')\n",
    "\n",
    "    # truncate unused columns\n",
    "    df_sold = df_sold[['productid', 'date', 'retailprice']]\n",
    "    # drop duplicated rows \n",
    "    df_sold = df_sold.drop_duplicates()\n",
    "\n",
    "    print(df_sold.shape)\n",
    "    print(df_sold.head())\n",
    "\n",
    "    df_merged = pd.merge(df_sold, df2, on = ['productid', 'date'], how = 'left')\n",
    "\n",
    "    df_merged['discount_percentage'] = df_merged['saleprice'].fillna(0) / df_merged['retailprice'].fillna(0)\n",
    "    \n",
    "    df_truncated = df_merged[['productid', 'date', 'discount_percentage']]\n",
    "\n",
    "    # rename columns \n",
    "    df_truncated = df_truncated.rename({'productid': 'PID', 'date':'date_time'}, axis=1)\n",
    " \n",
    "    # map dateID column from date data\n",
    "    df_mapped = pd.merge(df_truncated, date_map, on = ['date_time'], how = 'left')\n",
    "    # remove date_time column\n",
    "    df_clean = df_mapped[['PID', 'dateID', 'discount_percentage']]\n",
    "\n",
    "    print(df_clean.head())\n",
    "    print(df_clean.shape[0])\n",
    "    \n",
    "    # discount_percentage = 0 also shows in OnSale table\n",
    "    # print(df_clean.loc[df_clean['discount_percentage'] == 0, :].head())\n",
    "    print(df_clean.loc[df_clean['discount_percentage'] == 0, :].shape[0])\n",
    "\n",
    "    print(df_clean.loc[df_clean['discount_percentage'] > 0, :].shape[0]) # matched with product_sale_prices.tsv\n",
    "\n",
    "    # write into sample_data folder\n",
    "    df_clean.to_csv(os.path.join(curr_dir, 'OnSale.csv'), index=False)\n",
    "\n",
    "transform_onsale_data('product_sales.tsv', 'product_sale_prices.tsv', 'product.tsv', output_date_data('date.tsv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "   productid  storeid        date  quantity\n",
      "0          1      108  2007-01-21         1\n",
      "1          1      214  2002-09-02         7\n",
      "2          1      234  2002-05-11         2\n",
      "3          1      242  2000-09-14         1\n",
      "4          1      315  2005-02-27         3\n",
      "(500000, 4)\n",
      "(499023, 2)\n",
      "(499894, 3)\n"
     ]
    }
   ],
   "source": [
    "# validate product_sales.tsv \n",
    "product_sales_df = pd.read_csv(os.path.join(orig_sample_data_dir, 'product_sales.tsv'), sep='\\t')\n",
    "print(product_sales_df.head())\n",
    "print(product_sales_df.shape)\n",
    "\n",
    "product_sales_df1 = product_sales_df[['productid', 'date']].drop_duplicates()\n",
    "print(product_sales_df1.shape)\n",
    "\n",
    "product_sales_df2 = product_sales_df[['productid', 'date', 'quantity']].drop_duplicates()\n",
    "print(product_sales_df2.shape)\n",
    "\n",
    "\n",
    "#membership_df.groupby('membershiptype')['membershipid'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}